{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9erOxTWeVmPt"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"center\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/practicaldl/Practical-Deep-Learning-Book/blob/master/code/chapter-2/2-what-does-my-neural-network-think.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "This code is part of [Chapter 2 - What’s in the Picture: Image Classification with Keras](https://learning.oreilly.com/library/view/practical-deep-learning/9781492034858/ch02.html). This notebook will not run on Colab. For Colab, use <a target=\"_blank\" href=\"https://github.com/practicaldl/Practical-Deep-Learning-Book/blob/master/code/chapter-2/2-colab-what-does-my-neural-network-think.ipynb\">chapter-2/2-colab-what-does-my-neural-network-think.ipynb</a> instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIw8L7yWVmP0"
   },
   "source": [
    "# What Does My Neural Network Think?\n",
    "\n",
    "In this code sample, we try to understand why the neural network made a particular prediction. We use visualization (a heatmap) to understand the decision-making that is going on within the network. Using color, we visually identify the areas within an image that prompted a decision. “Hot” spots, represented by warmer colors (red, orange, and yellow) highlight the areas with the maximum signal, whereas cooler colors (blue, purple) indicate low signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKJnYNagVmP1"
   },
   "source": [
    "The `visualization.py` script produces the heatmap for one or more input images, overlays it on the image, and stitches it side-by-side with the original image for comparison. The script accepts arguments for image path or a directory that contains frames of a video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOH5z9wLVmP2"
   },
   "source": [
    "## Visualizing the Heatmap of an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IQNdCWU2VmP3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-25 20:48:50.910607: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Figure(640x480)\n"
     ]
    }
   ],
   "source": [
    "!python /Users/nguyenvananh/Downloads/data/visualization.py --process image --path /Users/nguyenvananh/Downloads/data/dog.jpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd8rCKEPVmP6"
   },
   "source": [
    "![t](https://github.com/PracticalDL/Practical-Deep-Learning-Book/blob/master/code/chapter-2/data/dog-output.jpg?raw=1)\n",
    "The right half of the image indicates the “areas of heat” along with the correct prediction of a 'Cardigan Welsh Corgi'.\n",
    "\n",
    "Note: As we can see below, the label is different from the labels shown in the book. This is because we use the [VGG-19](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) model in the visualization script, whereas we used the [ResNet-50](https://github.com/KaimingHe/deep-residual-networks) model in the book.\n",
    "\n",
    "![t](https://github.com/PracticalDL/Practical-Deep-Learning-Book/blob/master/code/chapter-2/data/cat-output.jpg?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xD68jDaqVmP7"
   },
   "source": [
    "## Visualizing the Heatmap of a Video\n",
    "\n",
    "Before we can run the `visualization.py` script, we will need to use `ffmpeg` to split up a video into individual frames. Let's create a directory to store these frames and pass its name as an argument into the `ffmpeg` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Li7lLrlBVmP8"
   },
   "outputs": [],
   "source": [
    "!mkdir kitchen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WrOepSlbVmP8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'IMG_7576.MOV':\n",
      "  Metadata:\n",
      "    major_brand     : qt  \n",
      "    minor_version   : 0\n",
      "    compatible_brands: qt  \n",
      "    creation_time   : 2022-05-25T19:26:04.000000Z\n",
      "    com.apple.quicktime.location.accuracy.horizontal: 12.775215\n",
      "    com.apple.quicktime.location.ISO6709: +52.4966+013.5142+039.445/\n",
      "    com.apple.quicktime.make: Apple\n",
      "    com.apple.quicktime.model: iPhone 11\n",
      "    com.apple.quicktime.software: 15.4.1\n",
      "    com.apple.quicktime.creationdate: 2022-05-25T21:26:04+0200\n",
      "  Duration: 00:00:12.51, start: 0.000000, bitrate: 7967 kb/s\n",
      "  Stream #0:0[0x1](und): Video: hevc (Main) (hvc1 / 0x31637668), yuv420p(tv, bt709), 1920x1080, 7714 kb/s, 29.97 fps, 29.97 tbr, 600 tbn (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2022-05-25T19:26:04.000000Z\n",
      "      handler_name    : Core Media Video\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : HEVC\n",
      "    Side data:\n",
      "      displaymatrix: rotation of -90.00 degrees\n",
      "  Stream #0:1[0x2](und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 197 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2022-05-25T19:26:04.000000Z\n",
      "      handler_name    : Core Media Audio\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:2[0x3](und): Data: none (mebx / 0x7862656D), 0 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2022-05-25T19:26:04.000000Z\n",
      "      handler_name    : Core Media Metadata\n",
      "  Stream #0:3[0x4](und): Data: none (mebx / 0x7862656D), 0 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2022-05-25T19:26:04.000000Z\n",
      "      handler_name    : Core Media Metadata\n",
      "  Stream #0:4[0x5](und): Data: none (mebx / 0x7862656D), 34 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2022-05-25T19:26:04.000000Z\n",
      "      handler_name    : Core Media Metadata\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (hevc (native) -> mjpeg (native))\n",
      "Press [q] to stop, [?] for help\n",
      "\u001b[1;34m[swscaler @ 0x7ff299b9f000] \u001b[0m\u001b[1;34m[swscaler @ 0x7ff299bac000] \u001b[0m\u001b[0;33mdeprecated pixel format used, make sure you did set range correctly\n",
      "\u001b[0m\u001b[1;34m[swscaler @ 0x7ff299b9f000] \u001b[0m\u001b[1;34m[swscaler @ 0x7ff299bb9000] \u001b[0m\u001b[0;33mdeprecated pixel format used, make sure you did set range correctly\n",
      "\u001b[0m\u001b[1;34m[swscaler @ 0x7ff299b9f000] \u001b[0m\u001b[1;34m[swscaler @ 0x7ff299bc6000] \u001b[0m\u001b[0;33mdeprecated pixel format used, make sure you did set range correctly\n",
      "\u001b[0m\u001b[1;34m[swscaler @ 0x7ff299b9f000] \u001b[0m\u001b[1;34m[swscaler @ 0x7ff299bd3000] \u001b[0m\u001b[0;33mdeprecated pixel format used, make sure you did set range correctly\n",
      "\u001b[0m\u001b[1;34m[swscaler @ 0x7ff299b9f000] \u001b[0m\u001b[1;34m[swscaler @ 0x7ff299be0000] \u001b[0m\u001b[0;33mdeprecated pixel format used, make sure you did set range correctly\n",
      "\u001b[0mOutput #0, image2, to 'kitchen/thumb%04d.jpg':\n",
      "  Metadata:\n",
      "    major_brand     : qt  \n",
      "    minor_version   : 0\n",
      "    compatible_brands: qt  \n",
      "    com.apple.quicktime.creationdate: 2022-05-25T21:26:04+0200\n",
      "    com.apple.quicktime.location.accuracy.horizontal: 12.775215\n",
      "    com.apple.quicktime.location.ISO6709: +52.4966+013.5142+039.445/\n",
      "    com.apple.quicktime.make: Apple\n",
      "    com.apple.quicktime.model: iPhone 11\n",
      "    com.apple.quicktime.software: 15.4.1\n",
      "    encoder         : Lavf59.16.100\n",
      "  Stream #0:0(und): Video: mjpeg, yuvj420p(pc, bt709, progressive), 1080x1920, q=2-31, 200 kb/s, 25 fps, 25 tbn (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2022-05-25T19:26:04.000000Z\n",
      "      handler_name    : Core Media Video\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc59.18.100 mjpeg\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: N/A\n",
      "      displaymatrix: rotation of -0.00 degrees\n",
      "frame=  313 fps= 43 q=24.8 Lsize=N/A time=00:00:12.52 bitrate=N/A speed=1.72x    \n",
      "video:12404kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\n"
     ]
    }
   ],
   "source": [
    "!ffmpeg -i IMG_7576.MOV -vf fps=25 kitchen/thumb%04d.jpg -hide_banner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETwII_tOVmP9"
   },
   "source": [
    "Now let's run the `visualization.py` script with the path of the directory containing the frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "y0Q5h8B9VmP-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-25 21:27:47.165412: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n"
     ]
    }
   ],
   "source": [
    "!python /Users/nguyenvananh/Downloads/visualization.py --process video --path kitchen/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Y8HpLtsVmP-"
   },
   "source": [
    "Compile a video from those frames using ffmpeg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "KWhyJEYlVmP_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 5.0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with Apple clang version 12.0.0 (clang-1200.0.32.29)\n",
      "  configuration: --prefix=/usr/local/Cellar/ffmpeg/5.0.1 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags= --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libbluray --enable-libdav1d --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox\n",
      "  libavutil      57. 17.100 / 57. 17.100\n",
      "  libavcodec     59. 18.100 / 59. 18.100\n",
      "  libavformat    59. 16.100 / 59. 16.100\n",
      "  libavdevice    59.  4.100 / 59.  4.100\n",
      "  libavfilter     8. 24.100 /  8. 24.100\n",
      "  libswscale      6.  4.100 /  6.  4.100\n",
      "  libswresample   4.  3.100 /  4.  3.100\n",
      "  libpostproc    56.  3.100 / 56.  3.100\n",
      "Input #0, image2, from 'kitchen_output/result-%04d.jpg':\n",
      "  Duration: 00:00:12.52, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Video: mjpeg (Baseline), yuvj420p(pc, bt470bg/unknown/unknown), 1080x1920 [SAR 1:1 DAR 9:16], 25 fps, 25 tbr, 25 tbn\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "\u001b[1;36m[libx264 @ 0x7fbcca608d00] \u001b[0musing SAR=1/1\n",
      "\u001b[1;36m[libx264 @ 0x7fbcca608d00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "\u001b[1;36m[libx264 @ 0x7fbcca608d00] \u001b[0mprofile High, level 4.0, 4:2:0, 8-bit\n",
      "\u001b[1;36m[libx264 @ 0x7fbcca608d00] \u001b[0m264 - core 163 r3060 5db6aa6 - H.264/MPEG-4 AVC codec - Copyleft 2003-2021 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'kitchen/kitchen-output.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf59.16.100\n",
      "  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuvj420p(pc, bt470bg/unknown/unknown, progressive), 1080x1920 [SAR 1:1 DAR 9:16], q=2-31, 25 fps, 12800 tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc59.18.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "frame=  313 fps=7.1 q=-1.0 Lsize=   12689kB time=00:00:12.40 bitrate=8383.1kbits/s speed=0.279x     \n",
      "video:12685kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.035082%\n",
      "\u001b[1;36m[libx264 @ 0x7fbcca608d00] \u001b[0mframe I:6     Avg QP:21.26  size: 55024\n",
      "\u001b[1;36m[libx264 @ 0x7fbcca608d00] \u001b[0mframe P:117   Avg QP:23.41  size: 40496\n",
      "\u001b[1;36m[libx264 @ 0x7fbcca608d00] \u001b[0mframe B:190   Avg QP:24.27  size: 41687\n",
      "\u001b[1;36m[libx264 @ 0x7fbcca608d00] \u001b[0mconsecutive B-frames: 14.7%  9.6% 10.5% 65.2%\n",
      "\u001b[1;36m[libx264 @ 0x7fbcca608d00] \u001b[0mmb I  I16..4: 30.4% 69.2%  0.4%\n",
      "\u001b[1;36m[libx264 @ 0x7fbcca608d00] \u001b[0mmb P  I16..4: 32.7% 63.7%  0.2%  P16..4:  1.8%  0.6%  0.3%  0.0%  0.0%    skip: 0.6%\n",
      "\u001b[1;36m[libx264 @ 0x7fbcca608d00] \u001b[0mmb B  I16..4: 20.2% 55.4%  0.1%  B16..8:  7.2%  3.6%  1.2%  direct: 5.6%  skip: 6.7%  L0:48.1% L1:42.8% BI: 9.2%\n",
      "\u001b[1;36m[libx264 @ 0x7fbcca608d00] \u001b[0m8x8 transform intra:69.9% inter:91.1%\n",
      "\u001b[1;36m[libx264 @ 0x7fbcca608d00] \u001b[0mcoded y,uvDC,uvAC intra: 48.6% 50.7% 0.4% inter: 37.4% 52.3% 0.4%\n",
      "\u001b[1;36m[libx264 @ 0x7fbcca608d00] \u001b[0mi16 v,h,dc,p: 39% 32% 22%  7%\n",
      "\u001b[1;36m[libx264 @ 0x7fbcca608d00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 23% 25% 44%  3%  1%  0%  1%  1%  4%\n",
      "\u001b[1;36m[libx264 @ 0x7fbcca608d00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 44% 26% 13%  2%  4%  3%  5%  2%  2%\n",
      "\u001b[1;36m[libx264 @ 0x7fbcca608d00] \u001b[0mi8c dc,h,v,p: 44% 26% 23%  7%\n",
      "\u001b[1;36m[libx264 @ 0x7fbcca608d00] \u001b[0mWeighted P-Frames: Y:20.5% UV:17.9%\n",
      "\u001b[1;36m[libx264 @ 0x7fbcca608d00] \u001b[0mref P L0: 37.2% 11.5% 30.2% 19.4%  1.8%\n",
      "\u001b[1;36m[libx264 @ 0x7fbcca608d00] \u001b[0mref B L0: 70.2% 23.3%  6.5%\n",
      "\u001b[1;36m[libx264 @ 0x7fbcca608d00] \u001b[0mref B L1: 89.1% 10.9%\n",
      "\u001b[1;36m[libx264 @ 0x7fbcca608d00] \u001b[0mkb/s:8299.50\n"
     ]
    }
   ],
   "source": [
    "!ffmpeg -framerate 25 -i kitchen_output/result-%04d.jpg kitchen/kitchen-output.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPv-Guq1VmQA"
   },
   "source": [
    "[![Heatmap Demo Video](https://github.com/PracticalDL/Practical-Deep-Learning-Book/blob/master/code/chapter-2/data/kitchen-output/result_0001.jpg?raw=1)](https://youtu.be/DhMzvbYjkUY \"Chapter 2 - Heatmap Demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaS8guQoVmQB"
   },
   "source": [
    "Perfect! Imagine generating heatmaps to analyze the strong points and shortfalls of your trained model or a pretrained model. Don't forget to post your videos on Twitter with the hashtag [#PracticalDL](https://twitter.com/hashtag/PracticalDL)!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "2-what-does-my-neural-network-think.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
